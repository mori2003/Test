version: '3.8'

# AI Agent Showcase - Multi-Service Architecture
# ==============================================
# This Docker Compose file demonstrates a comprehensive
# microservices setup with various technologies and patterns

services:
  # Frontend Application
  frontend:
    image: nginx:alpine
    container_name: showcase_frontend
    ports:
      - "3000:80"
    volumes:
      - ./frontend:/usr/share/nginx/html:ro
      - ./config/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api
      - websocket
    networks:
      - frontend_network
      - backend_network
    environment:
      - NGINX_ENVSUBST_TEMPLATE_DIR=/etc/nginx/templates
      - NGINX_ENVSUBST_OUTPUT_DIR=/etc/nginx/conf.d
      - API_HOST=api
      - API_PORT=8000
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.frontend.rule=Host(`localhost`)"
      - "traefik.http.services.frontend.loadbalancer.server.port=80"

  # API Service
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
      args:
        - PYTHON_VERSION=3.11
        - ENVIRONMENT=production
    container_name: showcase_api
    ports:
      - "8000:8000"
    volumes:
      - ./api:/app:ro
      - ./data:/app/data
      - api_logs:/app/logs
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    networks:
      - backend_network
      - database_network
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=postgresql://showcase_user:secure_password@database:5432/showcase_db
      - REDIS_URL=redis://redis:6379/0
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-your-secret-key-here}
      - CORS_ORIGINS=http://localhost:3000,https://localhost:3000
      - LOG_LEVEL=INFO
      - WORKER_PROCESSES=4
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # WebSocket Service
  websocket:
    build:
      context: ./websocket
      dockerfile: Dockerfile
    container_name: showcase_websocket
    ports:
      - "8080:8080"
    depends_on:
      - redis
    networks:
      - backend_network
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379/1
      - CORS_ORIGIN=http://localhost:3000
      - WS_PORT=8080
    healthcheck:
      test: ["CMD", "node", "healthcheck.js"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Background Worker
  worker:
    build:
      context: ./api
      dockerfile: Dockerfile.worker
    container_name: showcase_worker
    volumes:
      - ./api:/app:ro
      - ./data:/app/data
      - worker_logs:/app/logs
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend_network
      - database_network
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/2
      - CELERY_RESULT_BACKEND=redis://redis:6379/3
      - DATABASE_URL=postgresql://showcase_user:secure_password@database:5432/showcase_db
      - LOG_LEVEL=INFO
    command: celery -A app.celery worker --loglevel=info --concurrency=4
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # Task Scheduler
  scheduler:
    build:
      context: ./api
      dockerfile: Dockerfile.worker
    container_name: showcase_scheduler
    volumes:
      - ./api:/app:ro
    depends_on:
      - redis
      - worker
    networks:
      - backend_network
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/2
      - CELERY_RESULT_BACKEND=redis://redis:6379/3
    command: celery -A app.celery beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    restart: unless-stopped

  # Database
  database:
    image: postgres:15-alpine
    container_name: showcase_database
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./config/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks:
      - database_network
    environment:
      - POSTGRES_DB=showcase_db
      - POSTGRES_USER=showcase_user
      - POSTGRES_PASSWORD=secure_password
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U showcase_user -d showcase_db"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  # Redis Cache & Message Broker
  redis:
    image: redis:7-alpine
    container_name: showcase_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./config/redis.conf:/etc/redis/redis.conf:ro
    networks:
      - backend_network
    command: redis-server /etc/redis/redis.conf
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M

  # Elasticsearch for Search & Analytics
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: showcase_elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - backend_network
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - bootstrap.memory_lock=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G

  # Kibana for Elasticsearch Visualization
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: showcase_kibana
    ports:
      - "5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - backend_network
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - SERVER_NAME=kibana
      - SERVER_HOST=0.0.0.0
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # Monitoring with Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: showcase_prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - monitoring_network
      - backend_network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  # Grafana for Metrics Visualization
  grafana:
    image: grafana/grafana:latest
    container_name: showcase_grafana
    ports:
      - "3001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - monitoring_network
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Load Balancer / Reverse Proxy
  traefik:
    image: traefik:v2.10
    container_name: showcase_traefik
    ports:
      - "80:80"
      - "8081:8080"  # Traefik dashboard
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/traefik.yml:/etc/traefik/traefik.yml:ro
    networks:
      - frontend_network
      - backend_network
    command:
      - --api.insecure=true
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --entrypoints.web.address=:80
    restart: unless-stopped

  # Message Queue (Alternative to Redis for complex workflows)
  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: showcase_rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"  # Management UI
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    networks:
      - backend_network
    environment:
      - RABBITMQ_DEFAULT_USER=showcase
      - RABBITMQ_DEFAULT_PASS=secure_password
      - RABBITMQ_DEFAULT_VHOST=/showcase
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # File Storage (MinIO - S3 Compatible)
  minio:
    image: minio/minio:latest
    container_name: showcase_minio
    ports:
      - "9000:9000"
      - "9001:9001"  # Console
    volumes:
      - minio_data:/data
    networks:
      - backend_network
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin123
      - MINIO_BROWSER_REDIRECT_URL=http://localhost:9001
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    restart: unless-stopped

# Networks
networks:
  frontend_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
  backend_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16
  database_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/16
  monitoring_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.23.0.0/16

# Volumes
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  elasticsearch_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  rabbitmq_data:
    driver: local
  minio_data:
    driver: local
  api_logs:
    driver: local
  worker_logs:
    driver: local

# Secrets (for production use)
secrets:
  jwt_secret:
    file: ./secrets/jwt_secret.txt
  db_password:
    file: ./secrets/db_password.txt
  api_key:
    file: ./secrets/api_key.txt